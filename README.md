# MASAC_env_origin
初始版
这个项目利用强化学习（具体为SAC算法）来训练多个智能体在一个带有障碍物的二维环境中，从各自的起点移动到终点，并在此过程中学习如何避免相互碰撞以及与环境边界的碰撞。

项目主要由以下四个Python脚本文件组成：
env.py: 定义了核心的仿真环境。
astar.py: 实现了A*算法，为智能体提供全局路径参考。
train_sb3.py: 负责使用Stable Baselines3库来训练强化学习模型。
replay.py: 用于加载训练好的模型，进行可视化回放和性能分析。

详细代码功能解析
1. env.py (环境定义)
该文件定义了一个名为 MultiAgentPathFindingEnv 的自定义Gymnasium环境，它是整个项目的核心。

分阶段控制:
巷道阶段 (tunnel): 智能体初始位于一个狭窄的“巷道”中。在此阶段，它们不受强化学习模型控制，而是遵循一个简单的规则：以固定的速度（tunnel_speed）朝预设的巷道出口（exit_positions）移动。
强化学习阶段 (rl): 一旦智能体到达巷道出口，其控制权便交由强化学习模型接管。模型会根据当前状态输出动作，引导智能体绕过障碍，最终到达目标点。

观察空间 (Observation Space):
为了做出决策，每个智能体在RL阶段会接收到丰富的状态信息，包括：自身和目标的位置。其他所有智能体的当前位置和速度。由A*算法预先计算出的参考路径上的多个路标点（Waypoints）的相对位置和距离。

动作空间 (Action Space):
这是一个连续的动作空间，每个智能体可以决定两件事：1.前进速度: 控制移动的快慢。2.转向速率: 控制方向的改变。

奖励函数 (Reward Function):
奖励函数的设计非常关键，它引导着智能体的学习方向。该项目中的奖励包含多个组成部分：
1.前进奖励: 当智能体向目标点靠近时，给予正奖励。
2.生存奖励/停滞惩罚: 持续给予少量正奖励以鼓励探索，若停滞不前则给予惩罚。
3.A*路径引导奖励: 当智能体贴近A*算法给出的参考路径时，给予额外奖励；偏离过远则给予惩罚。
4.到达目标奖励: 成功到达终点时，给予一个巨大的正奖励。
5.碰撞惩罚: 与其他智能体或障碍物发生碰撞时，给予巨大的负奖励，并提前结束当前回合。
6.平滑度惩罚: 对速度和角速度的剧烈变化进行惩罚，鼓励智能体平稳移动。
7.时间惩罚: 每走一步都会有一个小的负奖励，促使智能体尽快完成任务。

渲染功能:
环境支持两种渲染模式：human模式可以实时显示智能体的移动过程；rgb_array模式则可以将当前帧渲染为图像数据，用于回调函数中保存评估过程的快照。

2. astar.py (A*路径规划)
这个文件实现了经典的A*寻路算法，它在项目中扮演着“全局导航员”的角色。
1.路径计算: 当智能体完成“巷道阶段”后，会调用A*算法计算一条从巷道出口到最终目标点的最优路径。这条路径会绕开所有已知的静态障碍物。
2.特征提取: A*计算出的路径本身并不会直接控制智能体，而是被处理成一系列的“路标点”，作为观察空间的一部分输入给强化学习模型。这为智能体提供了关于全局路径方向的先验知识，极大地降低了学习难度。

3. train_sb3.py (模型训练)
该脚本是训练流程的入口。它使用业界知名的强化学习库 Stable Baselines3 来训练模型。

环境包装器 (SingleAgentWrapper): Stable Baselines3主要针对单智能体环境设计。为了训练这个多智能体环境，代码使用了一个包装器，将多个智能体的观察和动作拼接在一起，从模型的角度“伪装”成一个具有超大观察空间和动作空间的单智能体环境。这是一种简单有效的集中式训练方法。
SAC算法: 脚本选用了**Soft Actor-Critic (SAC)**算法，这是一种高效的、适用于连续动作空间的现代强化学习算法。脚本中配置了SAC的各类超参数，如学习率、折扣因子gamma、缓冲区大小等。
随机种子: 为了保证实验的可复现性，代码在Numpy和PyTorch库以及模型和环境创建时都设置了固定的随机种子（SEED = 26）。
自定义回调 (CustomCallback): 在训练过程中，这个回调函数会周期性地在独立的评估环境中测试模型的当前性能。如果模型的平均奖励超过了历史最佳记录，它会自动将当前最好的模型权重保存到sb3_best_model3/目录下，并保存评估过程的截图到evaluation_images3/。

4. replay.py (策略回放与分析)
当模型训练完成后，可以使用此脚本来检验训练成果。

1.加载模型: 脚本会从sb3_best_model3/best_model.zip加载训练好的模型。
2.可视化回放: 在human模式下渲染环境，直观地展示智能体的决策过程和路径轨迹。
3.数据收集与绘图: 在回放的同时，脚本会记录每个智能体随时间变化的位置和速度数据。在一个回合结束后，它会利用这些数据绘制并保存两张图表：

时空曲线图 (spacetime_curves.png): 展示每个智能体在X和Y轴上的坐标随时间的变化。
速度曲线图 (velocity_curves.png): 展示每个智能体的速率随时间的变化。

总而言之，这个项目通过结合经典的A*算法和先进的深度强化学习技术，构建了一个完整的多智能体路径规划解决方案，并包含了从环境定义、模型训练到性能评估与可视化的全过程。
